# -*- coding: utf-8 -*-
"""Copy of MovieRecommenderSystemv2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1I2eMxQff63HUhRuWb2DRRXs094IouhSj

# 1. Importing Relevant Libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import random
from sklearn.metrics.pairwise import cosine_similarity
from scipy.sparse import csr_matrix
from sklearn.neighbors import NearestNeighbors
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.metrics import recall_score
from sklearn.metrics import precision_score
from sklearn.metrics import f1_score
import seaborn as sns

from google.colab import files
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

"""# 2. Importing MovieLens Dataset"""

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

#20m_750users_750movies
#https://drive.google.com/file/d/1ojdy3h32jevDcsLurXMvrZ4Zy5I4M_z2/view?usp=sharing (User-ratings-dataset)
User_ratings_file_id = '1ojdy3h32jevDcsLurXMvrZ4Zy5I4M_z2' 
User_ratings_downloaded = drive.CreateFile({'id': User_ratings_file_id})
print('Downloaded content "{}"'.format(User_ratings_downloaded.GetContentString()))

User_ratings_downloaded.GetContentFile('20m_750users_750movies.csv')

#https://drive.google.com/file/d/1iXyiV89uAJUYTPG99MNh-5RtYhPeNzR4/view?usp=sharing (movie-dataset)
movies_file_id = '1iXyiV89uAJUYTPG99MNh-5RtYhPeNzR4' 
movies_downloaded = drive.CreateFile({'id': movies_file_id})
print('Downloaded content "{}"'.format(movies_downloaded.GetContentString()))

movies_downloaded.GetContentFile('movies.csv')

#First 750 users and first 750 movies from the original dataset.
#Dataset file too large to import the whole thing.
#Ensure the target attribute is at the last column. If not, make it the last column.
#user_ratings_df = pd.read_csv("20m_5000users_7000movies.csv")
#user_ratings_df = pd.read_csv("20m-5000users-2000movies.csv")
user_ratings_df = pd.read_csv("20m_750users_750movies.csv")

user_ratings_df = user_ratings_df.drop(['timestamp'], 1) 
user_ratings_df

movies_df = pd.read_csv("movies.csv")
movies_df = movies_df.drop(['genres'], 1) #Remove redundant attribute
movies_df

#Check for duplicates

has_duplicate = user_ratings_df.duplicated()
duplicates = user_ratings_df[has_duplicate]
print(duplicates)

has_duplicate = user_ratings_df.duplicated()
duplicates = user_ratings_df[has_duplicate]
print(duplicates)

"""# 3. Train-Test-Data Split"""

train_df, test_df = train_test_split(user_ratings_df, test_size=0.2)
train_df = train_df.reset_index(drop = True)
test_df = test_df.reset_index(drop = True)

train_df

test_df

"""# 4. Item-item CF Algorithm"""

train_df_matrix = train_df.pivot(index='movieId',columns='userId',values='rating').fillna(0)
train_df_matrix

#Mitigate 'cold-start' problem
#Only allow movies which has been voted more than 15 times
#Only allow users who votes more than 20 times

freq_user_voted = train_df.groupby('movieId')['rating'].agg('count')
freq_movies_voted = train_df.groupby('userId')['rating'].agg('count')

f,ax = plt.subplots(1,1,figsize=(16,4))
plt.scatter(freq_user_voted.index,freq_user_voted,color='skyblue')
plt.axhline(y=10,color='r')
plt.xlabel('MovieId')
plt.ylabel('Freq. of users voted')
plt.show()

train_df_matrix = train_df_matrix.loc[freq_user_voted[freq_user_voted >= 10].index,:]
test_df = test_df.loc[test_df['movieId'].isin(freq_user_voted[freq_user_voted >= 10].index.to_list())]

f,ax = plt.subplots(1,1,figsize=(16,4))
plt.scatter(freq_movies_voted.index,freq_movies_voted,color='skyblue')
plt.axhline(y=10,color='r')
plt.xlabel('UserId')
plt.ylabel('Freq. of votes by user')
plt.show()

train_df_matrix = train_df_matrix.loc[:,freq_movies_voted[freq_movies_voted >= 10].index]
test_df = test_df.loc[test_df['userId'].isin(freq_movies_voted[freq_movies_voted >= 10].index.to_list())]

test_df

train_df_matrix

train_df_matrix_csr = csr_matrix(train_df_matrix.values)
train_df_matrix.reset_index(inplace=True)

#knn
#cosine-similarity
#train knn with training data

knn = NearestNeighbors(metric='cosine', algorithm='brute', n_neighbors=30, n_jobs=-1)
knn.fit(train_df_matrix_csr)

def get_movie_recommendation(movie_id, n_movies_to_recomend):
    movie_idx = train_df_matrix[train_df_matrix['movieId'] == movie_id].index[0]

    distances , indices = knn.kneighbors(train_df_matrix_csr[movie_idx],n_neighbors=n_movies_to_recomend+1)    
    rec_movie_indices = sorted(list(zip(indices.squeeze().tolist(),distances.squeeze().tolist())),\
                        key=lambda x: x[1])[:0:-1]
        
    recommend_frame = []
        
    for val in rec_movie_indices:
        movie_idx = train_df_matrix.iloc[val[0]]['movieId']
        idx = movies_df[movies_df['movieId'] == movie_idx].index
        recommend_frame.append({'Title':movies_df.iloc[idx]['title'].values[0],'Distance':val[1]})
    df = pd.DataFrame(recommend_frame,index=range(1,n_movies_to_recomend+1))
    return df

movies_df

get_movie_recommendation(1, 10)

#weighted average

def predict_ratings(userid, movieid, n_movie_recommend):
    total = 0
    total_weight = 0
    df = get_movie_recommendation(movieid, n_movie_recommend)
    for i in range(len(df)):
        movie_title = df.iloc[i]['Title']
        movie_id = movies_df.loc[movies_df['title'] == movie_title].iloc[0]['movieId']
        cos_sim = df.iloc[i]['Distance']
        match_df = train_df.loc[train_df['userId'] == userid]
        match_df = match_df.loc[match_df['movieId'] == movie_id]
        if (match_df.empty):
            continue
        else:
            movie_rating = match_df.iloc[0]['rating']
            total = total + (cos_sim * movie_rating)
            total_weight = total_weight + cos_sim
    
    if (total_weight == 0):
        pred_rate = 0
    else:
        #pred_rate = (total / df['Distance'].sum())
        pred_rate = (total / total_weight)
    return round(pred_rate, 2)

"""# 5. Test"""

#Test algorithm on random sample from test data

attributes_df = list(user_ratings_df.columns)
attributes_df.remove('rating')

test_x = test_df[attributes_df]
test_y = test_df['rating']

n_movies_to_recommend = 10
random_sample = random.randrange(len(test_x))
predicted_rating = predict_ratings(test_x.iloc[random_sample]['userId'], test_x.iloc[random_sample]['movieId'], n_movies_to_recommend)
print(test_x.iloc[random_sample])
print(f"Predicted rating: {predicted_rating}")
print(f"Truth: {test_y.values[random_sample]}")

"""# 6. Evaluate"""

#test_df.loc[test_df['movieId'] == 1412]

#RMSE
"""
def compute_rmse():
    test_y_array = test_df['rating'].to_list()
    predicted_array = []
    for i in range(len(test_df)):
    #for i in range(len(test_x)):
        predicted_array.append(predict_ratings(test_x.iloc[i]['userId'], test_x.iloc[i]['movieId']))
    predicted_array = np.asarray(predicted_array)
    #print(test_y_array[:27])
    #print(predicted_array)
    print(mean_squared_error(test_y_array, predicted_array, squared=False))

compute_rmse()
"""

# Add new column, likes
# ratings >= 3 -> 1
# ratings <3 -> 0
# 1 means user likes purchased movie
# 0 means user dislikes purchased movie


likes = (test_df['rating'] >= 3).to_list()
test_df['likes'] = likes
test_df['likes'] = test_df['likes'].astype(int)
test_df

#RSMD
#Recall
#Code roughly takes 50 mins to run
#Run code below for a faster evaluation

n_movies_to_recommend = 100

def compute_performance():
    test_y_array = test_df['rating'].to_list()
    predicted_array = []
    for i in range(len(test_df)):
    #for i in range(100):
        predicted_array.append(predict_ratings(test_x.iloc[i]['userId'], test_x.iloc[i]['movieId'],n_movies_to_recommend))
    predicted_array = np.asarray(predicted_array)
    rsmd = mean_squared_error(test_y_array, predicted_array, squared=False)
    #print(f"Truth Ratings:     {test_y_array}")
    #rsmd = mean_squared_error(test_y_array[:100], predicted_array, squared=False)
    #print(f"Truth Ratings:     {test_y_array[:100]}")
    #print(f"Predicted Ratings: {predicted_array}")
    print(f"RSMD: {rsmd}\n")

    for index, item in enumerate(predicted_array):
      if item >= 3:
        predicted_array[index] = 1
      else:
        predicted_array[index] = 0
  
    test_y2_array = test_df['likes'].to_list()
    #print(f"Truth:     {test_y2_array}")
    #print(f"Truth:     {test_y2_array[:100]}")
    #print(f"Predicted: {predicted_array}")

    #recall1 = recall_score(test_y2_array, predicted_array, average='macro')
    #recall2 = recall_score(test_y2_array, predicted_array, average='micro')
    recall3 = recall_score(test_y2_array, predicted_array, average='weighted')
    #recall1 = recall_score(test_y2_array[:100], predicted_array, average='macro')
    #recall2 = recall_score(test_y2_array[:100], predicted_array, average='micro')
    #recall3 = recall_score(test_y2_array[:100], predicted_array, average='weighted')   
    #print(f"Recall Score (Macro): {recall1}")
    #print(f"Recall Score (Micro): {recall2}")
    print(f"Recall Score (Weighted): {recall3}\n")

compute_performance()

#Faster performance evaluation.
#Only uses first 100 user-movie pair.
#Does not iterate through the whole test data.
#Takes roughly 1 min to run
#More or less the same performance

n_movies_to_recommend = 100

def compute_performance():
    test_y_array = test_df['rating'].to_list()
    predicted_array = []
    for i in range(100):
        predicted_array.append(predict_ratings(test_x.iloc[i]['userId'], test_x.iloc[i]['movieId'],n_movies_to_recommend))
    predicted_array = np.asarray(predicted_array)
    rsmd = mean_squared_error(test_y_array[:100], predicted_array, squared=False)
    #print(f"Truth Ratings:     {test_y_array[:100]}")
    #print(f"Predicted Ratings: {predicted_array}")
    print(f"RSMD: {rsmd}\n")

    for index, item in enumerate(predicted_array):
      if item >= 3:
        predicted_array[index] = 1
      else:
        predicted_array[index] = 0
  
    test_y2_array = test_df['likes'].to_list()
    #print(f"Truth:     {test_y2_array[:100]}")
    #print(f"Predicted: {predicted_array}")

    recall1 = recall_score(test_y2_array[:100], predicted_array, average='macro')
    recall2 = recall_score(test_y2_array[:100], predicted_array, average='micro')
    recall3 = recall_score(test_y2_array[:100], predicted_array, average='weighted')   
    print(f"Recall Score (Macro): {recall1}")
    print(f"Recall Score (Micro): {recall2}")
    print(f"Recall Score (Weighted): {recall3}\n")

compute_performance()

#Faster performance evaluation.
#Only uses first 10 user-movie pair.
#Does not iterate through the whole test data.

n_movies_to_recommend = 100

def compute_performance():
    test_y_array = test_df['rating'].to_list()
    predicted_array = []
    for i in range(10):
        predicted_array.append(predict_ratings(test_x.iloc[i]['userId'], test_x.iloc[i]['movieId'],n_movies_to_recommend))
    predicted_array = np.asarray(predicted_array)
    rsmd = mean_squared_error(test_y_array[:10], predicted_array, squared=False)
    #print(f"Truth Ratings:     {test_y_array[:10]}")
    #print(f"Predicted Ratings: {predicted_array}")
    print(f"RSMD: {rsmd}\n")

    for index, item in enumerate(predicted_array):
      if item >= 3:
        predicted_array[index] = 1
      else:
        predicted_array[index] = 0
  
    test_y2_array = test_df['likes'].to_list()
    #print(f"Truth:     {test_y2_array[:10]}")
    #print(f"Predicted: {predicted_array}")

    recall1 = recall_score(test_y2_array[:10], predicted_array, average='macro')
    recall2 = recall_score(test_y2_array[:10], predicted_array, average='micro')
    recall3 = recall_score(test_y2_array[:10], predicted_array, average='weighted')   
    print(f"Recall Score (Macro): {recall1}")
    print(f"Recall Score (Micro): {recall2}")
    print(f"Recall Score (Weighted): {recall3}\n")

compute_performance()

"""
    predicted_array = [round(num) for num in predicted_array]
    test_y_array = [round(num) for num in test_y_array]
    print(test_y_array[:10])
    print(predicted_array)
    recall1 = recall_score(test_y_array[:10], predicted_array, average='macro')
    recall2 = recall_score(test_y_array[:10], predicted_array, average='micro')
    recall3 = recall_score(test_y_array[:10], predicted_array, average='weighted')   
    print(f"Recall Score (Macro): {recall1}")
    print(f"Recall Score (Micro): {recall2}")
    print(f"Recall Score (Weighted): {recall3}\n")

    precision_score1 = precision_score(test_y_array[:10], predicted_array, average='macro')
    precision_score2 = precision_score(test_y_array[:10], predicted_array, average='micro')
    precision_score3 = precision_score(test_y_array[:10], predicted_array, average='weighted')   
    print(f"precision Score (Macro): {precision_score1}")
    print(f"Precision Score (Micro): {precision_score2}")
    print(f"Precision Score (Weighted): {precision_score3}\n")

    f1_score1 = f1_score(test_y_array[:10], predicted_array, average='macro')
    f1_score2 = f1_score(test_y_array[:10], predicted_array, average='micro')
    f1_score3 = f1_score(test_y_array[:10], predicted_array, average='weighted')   
    print(f"F1 Score (Macro): {f1_score1}")
    print(f"F1 Score (Micro): {f1_score2}")
    print(f"F1 Score (Weighted): {f1_score3}\n")
"""